{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import inception_blocks_v4\n",
    "import inception_blocks_v4_modified\n",
    "import glob\n",
    "import PIL\n",
    "#import inception.preprocessing\n",
    "import matplotlib.image as mpimg\n",
    "#from data_aug import *\n",
    "import os\n",
    "#from tensorflow import data\n",
    "from utils import *\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "#Pretrained_weights = \"/checkpoint_files/inception_v4.ckpt\" 3453\n",
    "ckpt_dir = \"checkpoint_files/inception_v4.ckpt\"\n",
    "new_ckpt_dir = \"checkpoints_files/model.ckpt\"\n",
    "image_size = 299\n",
    "images_file_path = \"C:/Users/ashus/Dog_classification/output/*.jpg\"\n",
    "labels_dir = \"labels.csv\"\n",
    "PATH = \"C:/Users/ashus/Dog_classification/output\"\n",
    "\n",
    "pre_num_classes = 1001\n",
    "num_classes = 120\n",
    "iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint_files/inception_v4.ckpt\n",
      "WARNING:tensorflow:From c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:691: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Tensor(\"batch:0\", shape=(1, 299, 299, 3), dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"batch:1\", shape=(1, 120), dtype=float32, device=/device:CPU:0)\n",
      "WARNING:tensorflow:From c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\slim\\python\\slim\\learning.py:736: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Restoring parameters from checkpoints_files/model.ckpt\\model.ckpt-0\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Saving checkpoint to path checkpoints_files/model.ckpt\\model.ckpt\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:Recording summary at step 0.\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [?,299,299,3]\n",
      "\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[?,299,299,3], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "\n",
      "Caused by op 'Placeholder', defined at:\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 832, in start\n",
      "    self._run_callback(self._callbacks.popleft())\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 605, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 536, in <lambda>\n",
      "    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-d5ad5bfdad38>\", line 5, in <module>\n",
      "    X_input = tf.placeholder(tf.float32, shape = (None, image_size, image_size, 3))\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1680, in placeholder\n",
      "    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 4105, in _placeholder\n",
      "    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"c:\\users\\ashus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [?,299,299,3]\n",
      "\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[?,299,299,3], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 0.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:CPU:0'):\n",
    "    #with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    with slim.arg_scope(inception_blocks_v4.inception_v4_arg_scope()):\n",
    "        X_input = tf.placeholder(tf.float32, shape = (None, image_size, image_size, 3))\n",
    "        Y_label = tf.placeholder(tf.float32, shape = (None, num_classes))\n",
    "            \n",
    "        targets = convert_to_onehot(labels_dir, no_of_features = num_classes)\n",
    "        assert targets.shape[1] == 120, 'THE TARGETS SHAPE IS NOT CORRECT'\n",
    "        targets = tf.constant(targets, dtype = tf.float32)\n",
    "\n",
    "        Images = [] #TO STORE THE RESIZED IMAGES IN THE FORM OF LIST TO PASS IT TO tf.train.batch()\n",
    "        images = glob.glob(images_file_path)\n",
    "        i = 0\n",
    "        for my_img in images:\n",
    "            image = mpimg.imread(my_img)[:, :, :3]\n",
    "            #print (image.shape)\n",
    "            #image = np.asarray(image, dtype = np.float32)\n",
    "            image = tf.constant(image, dtype = tf.float32)\n",
    "            Images.append(image)\n",
    "            i = i + 1\n",
    "            if i == 1:\n",
    "                break\n",
    "\n",
    "        #try:\n",
    "        #pretrained_weights = slim.assign_from_checkpoint_fn(ckpt_dir, slim.get_model_variables('InceptionV4'))\n",
    "        #with tf.Session() as sess:\n",
    "         #   pretrained_weights(sess)\n",
    "        #except ValueError:\n",
    "         #   print(\"The checkpoint file has some error\")\n",
    "\n",
    "        logits, end_points = inception_blocks_v4.inception_v4(inputs = X_input, num_classes = pre_num_classes, is_training = True, create_aux_logits= False)\n",
    "        pretrained_weights = slim.assign_from_checkpoint_fn(ckpt_dir, slim.get_model_variables('InceptionV4'))\n",
    "        with tf.Session() as sess:\n",
    "            pretrained_weights(sess)\n",
    "        \n",
    "        #MY LAYERS, add bias as well\n",
    "        my_layer = slim.fully_connected(logits, 560, activation_fn=tf.nn.relu, scope=None, weights_initializer = tf.truncated_normal_initializer(stddev = 0.001), weights_regularizer=slim.l2_regularizer(0.00005),biases_initializer = tf.truncated_normal_initializer(stddev=0.001), biases_regularizer=slim.l2_regularizer(0.00005))\n",
    "        my_layer = slim.dropout(my_layer, keep_prob = 0.6, scope = None)\n",
    "        my_layer = slim.fully_connected(my_layer, num_classes,activation_fn = tf.nn.relu,scope= None, weights_initializer = tf.truncated_normal_initializer(stddev=0.001), weights_regularizer=slim.l2_regularizer(0.00005), biases_initializer = tf.truncated_normal_initializer(stddev=0.001), biases_regularizer=slim.l2_regularizer(0.00005))\n",
    "        my_layer_logits = slim.fully_connected(my_layer, num_classes, activation_fn=None,scope=None)\n",
    "\n",
    "        #loss = tf.losses.mean_squared_error(labels = Y_label, predictions = predictions)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # Add the loss function to the graph.   \n",
    "        loss = tf.losses.softmax_cross_entropy(onehot_labels = Y_label, logits = my_layer_logits)  \n",
    "        #onehot_labels are the actual labels and logits are the predictions, we don't need to take the softmax of labels\n",
    "        # The total loss is the user's loss plus any regularization losses. slim.losses.get_total_loss() is deprecated\n",
    "        # to calc regularization losses use tf.losses.get_regularization_losses\n",
    "        #total_loss = tf.losses.get_total_loss()\n",
    "\n",
    "        # Specify the optimizer and create the train op:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        train_op = slim.learning.create_train_op(loss, optimizer) \n",
    "\n",
    "        #Generating batch\n",
    "        \n",
    "        batch_size = 1\n",
    "        images, labels = tf.train.batch([Images, targets], batch_size = 1, num_threads = 1, capacity = batch_size, enqueue_many=True)\n",
    "        tensor_images = tf.convert_to_tensor(images, dtype = tf.float32)\n",
    "        tensor_labels = tf.convert_to_tensor(labels, dtype = tf.float32)\n",
    "        #This function is generating (8,8,299,299,3). The first is my batch size\n",
    "        \n",
    "        #the below 2 lines is for countering images.shape =(8, 1, 299, 299, 3) problem\n",
    "        #images = tf.reshape(images, shape = (8, 299, 299, 3))\n",
    "        #labels = tf.reshape(labels, shape = (8, 120))\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            print (tensor_images)\n",
    "            print (tensor_labels)\n",
    "        #images, labels = load_batch(8, label = targets)\n",
    "        # Run the training inside a session.\n",
    "        final_loss = slim.learning.train(train_op,logdir = new_ckpt_dir, number_of_steps = iterations, save_summaries_secs=5,log_every_n_steps=50)(feed_dict = {X_input:tensor_images ,Y_label: tensor_labels})  #{X_input:images ,Y_label: labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
